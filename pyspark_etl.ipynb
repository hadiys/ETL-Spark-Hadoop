{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PySpark to initialize the SparkContext.   \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkContext object  \n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a SparkSession  \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract employee data from CSV, and transform using filtering, grouping, joins, and aggregations as a new DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the \"emp\" CSV file and import it into a DataFrame variable named \"employees_df\"  \n",
    "csv_file = 'data/employees.csv'\n",
    "employees_df = spark.read.load(csv_file, format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Schema for the input data and read the file using the user-defined Schema\n",
    "emp_schema = employees_df.schema\n",
    "employees_df = spark.read.load(csv_file, schema=emp_schema, format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp_No: string (nullable = true)\n",
      " |-- Emp_Name: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all columns of the DataFrame, along with their respective data types\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a temporary view named \"employees\" for the DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43memployees_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memployees\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.3/libexec/python/pyspark/sql/dataframe.py:370\u001b[0m, in \u001b[0;36mDataFrame.createTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.3/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.3/libexec/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views."
     ]
    }
   ],
   "source": [
    "# Create a temporary view named \"employees\" for the DataFrame\n",
    "employees_df.createTempView('employees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   200|   Jennifer|  4400| 36| Marketing|\n",
      "|   201|    Michael| 13000| 32|        IT|\n",
      "|   202|        Pat|  6000| 39|        HR|\n",
      "|   203|      Susan|  6500| 36| Marketing|\n",
      "|   205|    Shelley| 12008| 33|   Finance|\n",
      "|   206|    William|  8300| 37|        IT|\n",
      "|   100|     Steven| 24000| 39|        IT|\n",
      "|   102|        Lex| 17000| 37| Marketing|\n",
      "|   103|  Alexander|  9000| 39| Marketing|\n",
      "|   104|      Bruce|  6000| 38|        IT|\n",
      "|   105|      David|  4800| 39|        IT|\n",
      "|   106|      Valli|  4800| 38|     Sales|\n",
      "|   107|      Diana|  4200| 35|     Sales|\n",
      "|   109|     Daniel|  9000| 35|        HR|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   111|     Ismael|  7700| 32|        IT|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   113|       Luis|  6900| 34|     Sales|\n",
      "|   116|     Shelli|  2900| 37|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query to fetch solely the records from the View where the age exceeds 30\n",
    "result = spark.sql(\"SELECT * FROM employees WHERE age > 30\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to calculate the average salary of employees grouped by department\n",
    "result = spark.sql('SELECT Department, avg(Salary) as Average_salary FROM employees group by Department')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter to select records where the department is 'IT'\n",
    "result = employees_df.filter(employees_df.Department == 'IT')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
    "employees_df = employees_df.withColumn('SalaryAfterBonus', employees_df.Salary*1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Group data by age and calculate the maximum salary for each age group\n",
    "employees_df.groupBy('Age').agg(max('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
    "employees_df.join(employees_df, 'Emp_No', 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average age of employees\n",
    "from pyspark.sql.functions import avg \n",
    "\n",
    "employees_df.agg(avg('Age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      71408|\n",
      "|        HR|      46700|\n",
      "|   Finance|      57308|\n",
      "| Marketing|      59700|\n",
      "|        IT|      74000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total salary for each department.\n",
    "from pyspark.sql.functions import sum \n",
    "\n",
    "employees_df.groupBy('Department').agg(sum('Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "employees_df.sort(asc('Age'))\n",
    "employees_df.sort(desc('Salary'))\n",
    "\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Department|count(Emp_No)|\n",
      "+----------+-------------+\n",
      "|     Sales|           13|\n",
      "|        HR|            8|\n",
      "|   Finance|           10|\n",
      "| Marketing|            9|\n",
      "|        IT|           10|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calculate the number of employees in each department\n",
    "employees_df.groupBy('Department').agg(count('Emp_No')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+---+----------+\n",
      "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
      "+------+-----------+------+---+----------+\n",
      "|   198|     Donald|  2600| 29|        IT|\n",
      "|   199|    Douglas|  2600| 34|     Sales|\n",
      "|   110|       John|  8200| 31| Marketing|\n",
      "|   112|Jose Manuel|  7800| 34|        HR|\n",
      "|   130|      Mozhe|  2800| 28| Marketing|\n",
      "|   133|      Jason|  3300| 38|     Sales|\n",
      "|   139|       John|  2700| 36|     Sales|\n",
      "|   140|     Joshua|  2500| 29|   Finance|\n",
      "+------+-----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
    "employees_df.filter(employees_df.Emp_Name.like('%o%')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "prev_pub_hash": "aff0a6b4ec3a9cf15ae5d70a5c7ecac07e8a7f43b412a755232c9c99d1062fc8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
