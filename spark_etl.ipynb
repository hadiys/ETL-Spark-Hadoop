{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "# Ignore any warnings by SparkSession command\n",
    "spark = SparkSession.builder.appName(\"ETL using Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples, each tuple contains the student id, height and weight. Some rows are intentionally duplicated\n",
    "data = [(\"student1\",64,90),\n",
    "        (\"student2\",59,100),\n",
    "        (\"student3\",69,95),\n",
    "        (\"\",70,110),\n",
    "        (\"student5\",60,80),\n",
    "        (\"student3\",69,95),\n",
    "        (\"student6\",62,85),\n",
    "        (\"student7\",65,80),\n",
    "        (\"student7\",65,80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe using createDataFrame and pass the data and the column names.\n",
    "df = spark.createDataFrame(data, [\"student\",\"height_inches\",\"weight_pounds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student1|           64|           90|\n",
      "|student2|           59|          100|\n",
      "|student3|           69|           95|\n",
      "|        |           70|          110|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student6|           62|           85|\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the data frame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student-hw.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|student3|           69|           95|\n",
      "|student6|           62|           85|\n",
      "|    NULL|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the csv file\n",
    "df = spark.read.csv(\"student-hw.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Read from a csv file and write to parquet file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the number of rows in the dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|student6|           62|           85|\n",
      "|    NULL|           70|          110|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|student6|           62|           85|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save to parquet file\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3477 Jps',\n",
       " '2999 SparkSubmit',\n",
       " '1016 Worker',\n",
       " '955 Master',\n",
       " '3374 SparkSubmit']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!ls -l student-hw.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition data to a single partition\n",
    "df = df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save again as parquet as a single partition\n",
    "df.write.mode(\"overwrite\").parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l student-hw-single.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Parquet and save as CSV\n",
    "df = spark.read.parquet(\"student-hw-single.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+\n",
      "| student|height_inches|weight_pounds|\n",
      "+--------+-------------+-------------+\n",
      "|student7|           65|           80|\n",
      "|student2|           59|          100|\n",
      "|student1|           64|           90|\n",
      "|student3|           69|           95|\n",
      "|student5|           60|           80|\n",
      "|student6|           62|           85|\n",
      "+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the expr function that helps in transforming the data\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|\n",
      "+--------+-------------+-------------+------------------+\n",
      "|student7|           65|           80|            165.10|\n",
      "|student2|           59|          100|            149.86|\n",
      "|student1|           64|           90|            162.56|\n",
      "|student3|           69|           95|            175.26|\n",
      "|student5|           60|           80|            152.40|\n",
      "|student6|           62|           85|            157.48|\n",
      "+--------+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert inches to centimeters, multiplying with 2.54 to get a new column height_centimeters\n",
    "df = df.withColumn(\"height_centimeters\", expr(\"height_inches * 2.54\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert pounds to kilograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------+------------------+---------+\n",
      "| student|height_inches|weight_pounds|height_centimeters|weight_kg|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "|student7|           65|           80|            165.10|36.287360|\n",
      "|student2|           59|          100|            149.86|45.359200|\n",
      "|student1|           64|           90|            162.56|40.823280|\n",
      "|student3|           69|           95|            175.26|43.091240|\n",
      "|student5|           60|           80|            152.40|36.287360|\n",
      "|student6|           62|           85|            157.48|38.555320|\n",
      "+--------+-------------+-------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert pounds to kilograms\n",
    "df = df.withColumn(\"weight_kg\", expr(\"weight_pounds * 0.453592\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+---------+\n",
      "| student|height_centimeters|weight_kg|\n",
      "+--------+------------------+---------+\n",
      "|student7|            165.10|36.287360|\n",
      "|student2|            149.86|45.359200|\n",
      "|student1|            162.56|40.823280|\n",
      "|student3|            175.26|43.091240|\n",
      "|student5|            152.40|36.287360|\n",
      "|student6|            157.48|38.555320|\n",
      "+--------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the columns \"height_inches\",\"weight_pounds\"\n",
    "df = df.drop(\"height_inches\",\"weight_pounds\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "| student|height_cm|weight_kg|\n",
      "+--------+---------+---------+\n",
      "|student7|   165.10|36.287360|\n",
      "|student2|   149.86|45.359200|\n",
      "|student1|   162.56|40.823280|\n",
      "|student3|   175.26|43.091240|\n",
      "|student5|   152.40|36.287360|\n",
      "|student6|   157.48|38.555320|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the lengthy column name \"height_centimeters\" to \"height_cm\"\n",
    "df = df.withColumnRenamed(\"height_centimeters\",\"height_cm\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"student_transformed.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+\n",
      "| student|height_cm|weight_kg|\n",
      "+--------+---------+---------+\n",
      "|student7|    165.1| 36.28736|\n",
      "|student2|   149.86|  45.3592|\n",
      "|student1|   162.56| 40.82328|\n",
      "|student3|   175.26| 43.09124|\n",
      "|student5|    152.4| 36.28736|\n",
      "|student6|   157.48| 38.55532|\n",
      "+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the saved csv file\n",
    "df = spark.read.csv(\"student_transformed.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "prev_pub_hash": "39731b452d2aa53db85966a6e8def5ff45a8e2114b7123f9ebeb021e9b376712"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
